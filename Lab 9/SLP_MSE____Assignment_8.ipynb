{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLP-MSE || Assignment 8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSehflMxp3jF"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "\n",
        "\n",
        "def linear_predicted(x,w):\n",
        "  return np.dot(x,w)\n",
        "\n",
        "def logistic_predicted_sigmoid(x,w):\n",
        "  return 1/(1+ pow(math.e,-(np.dot(x,w))))\n",
        "  \n",
        "def linear_w_update(x,y,alpha,w):\n",
        "\n",
        "  for j in range(len(w)):\n",
        "    subtract_part = 0\n",
        "\n",
        "    for i in range(len(x)):\n",
        "      subtract_part = subtract_part + (linear_predicted(x[i],w)-y[i])*x[i][j]\n",
        "\n",
        "    w[j] = w[j] - (alpha*subtract_part)/len(x)\n",
        "\n",
        "  return w\n",
        "\n",
        "def logistic_w_update(x,y,alpha,w):\n",
        "\n",
        "  for j in range(len(w)):\n",
        "    subtract_part = 0\n",
        "\n",
        "    for i in range(len(x)):\n",
        "      subtract_part = subtract_part + (logistic_predicted_sigmoid(x[i],w)-y[i])*x[i][j]*(logistic_predicted_sigmoid(x[i],w))*(1-logistic_predicted_sigmoid(x[i],w))\n",
        "\n",
        "    w[j] = w[j] - (alpha*subtract_part)/len(x)\n",
        "\n",
        "  return w\n",
        "\n",
        "\n",
        "\n",
        "def error_linear(x,y,w):\n",
        "\n",
        "  J = 0\n",
        "  for i in range(len(x)):\n",
        "\n",
        "    J = J + pow((linear_predicted(x[i],w)-y[i]),2)\n",
        "\n",
        "  J = J/(2*len(x))\n",
        "\n",
        "  return J\n",
        "\n",
        "def error_logistic(x,y,w):\n",
        "  J = 0\n",
        "\n",
        "  for i in range(len(x)):\n",
        "    J = J + (y[i]*math.log(logistic_predicted_sigmoid(x[i],w),10)) + ((1-y[i])*math.log(1-logistic_predicted_sigmoid(x[i],w),10))\n",
        "\n",
        "  J = -J/(len(x))\n",
        "\n",
        "  return J\n",
        "\n",
        "\n",
        "\n",
        "def cal_MSE_batch(df,w,alpha,rho,epochs,showPlot):\n",
        "\n",
        "  a = np.array(df.values)\n",
        "\n",
        "  x = a[:,0:len(a[0])-1]\n",
        "\n",
        "  y = a[:,len(a[0])-1]\n",
        "\n",
        "  J = 0\n",
        "\n",
        "  prevJ = 0\n",
        "\n",
        "  itr = 0\n",
        "\n",
        "  epoch_graph = []\n",
        "  mse_graph = []\n",
        "\n",
        "  while True:\n",
        "\n",
        "    if(itr >= epochs):\n",
        "      break\n",
        "\n",
        "\n",
        "    h = []\n",
        "\n",
        "    \n",
        "\n",
        "    w = linear_w_update(x,y,alpha,w)\n",
        "    # w = logistic_w_update(x,y,alpha,w)\n",
        "\n",
        "    \n",
        "    J = 0\n",
        "\n",
        "    J = error_linear(x,y,w)\n",
        "\n",
        "    # print(J)\n",
        "    # J = error_logistic(x,y,w)\n",
        "\n",
        "    mse_graph.append(J)\n",
        "    epoch_graph.append(itr)\n",
        "\n",
        "\n",
        "    if(abs(J-prevJ) <= rho):\n",
        "      break\n",
        "\n",
        "    prevJ = J\n",
        "    itr = itr + 1\n",
        "\n",
        "  \n",
        "\n",
        "  if(showPlot==1):\n",
        "    plt.plot(epoch_graph,mse_graph)\n",
        "    plt.xlabel(\"EPOCHS\")\n",
        "    # naming the y axis\n",
        "    plt.ylabel(\"MSE\")\n",
        "    plt.show()\n",
        "  return w,J\n",
        "\n",
        "\n",
        "\n",
        "def cal_MSE_stoch(df,w,alpha,rho,epochs,showPlot):\n",
        "\n",
        "  a = np.array(df.values)\n",
        "\n",
        "  x = a[:,0:len(a[0])-1]\n",
        "\n",
        "  y = a[:,len(a[0])-1]\n",
        "\n",
        "  J = 0\n",
        "\n",
        "  prevJ = 0\n",
        "\n",
        "  itr = 0\n",
        "\n",
        "  epoch_graph = []\n",
        "  mse_graph = []\n",
        "\n",
        "  while True:\n",
        "\n",
        "    if(itr >= epochs):\n",
        "      break\n",
        "\n",
        "\n",
        "    h = []\n",
        "\n",
        "\n",
        "    for i in range(len(x)):\n",
        "\n",
        "      # predicted value must be changed\n",
        "      predicted_value = logistic_predicted_sigmoid(x[i],w)\n",
        "      # predicted_value = logistic_predicted_sigmoid(x[i],w)\n",
        "      h.append(predicted_value)\n",
        "\n",
        "      # gradient\n",
        "      # J = J + pow(predicted_value - y[i],2)\n",
        "\n",
        "      # stocastic - logloss\n",
        "      J = J + (y[i]*math.log(predicted_value,10)) + ((1-y[i])*math.log(1-predicted_value,10))\n",
        "\n",
        "      for j in range(len(w)):\n",
        "\n",
        "        # logloss-logistic, gradient-stocastic - MSE\n",
        "        w[j] = w[j] - alpha*(predicted_value-y[i])*x[i][j]\n",
        "\n",
        "        # logistic - MSE\n",
        "        # w[j] = w[j] - alpha*(predicted_value-y[i])*x[i][j]*predicted_value*(1-predicted_value)\n",
        "\n",
        "\n",
        "    # J = J/(2*len(x))\n",
        "\n",
        "    J = -J/(len(x))\n",
        "\n",
        "    \n",
        "\n",
        "    mse_graph.append(J)\n",
        "    epoch_graph.append(itr)\n",
        "\n",
        "    # shuffling.\n",
        "    df = df.sample(frac = 1)\n",
        "\n",
        "\n",
        "\n",
        "    if(abs(J-prevJ) <= rho):\n",
        "      break\n",
        "\n",
        "    prevJ = J\n",
        "    itr = itr + 1\n",
        "\n",
        "  \n",
        "  # print(mse_graph)\n",
        "  # print(epoch_graph)\n",
        "\n",
        "  if(showPlot != ''):\n",
        "    plt.plot(epoch_graph,mse_graph,label=str(showPlot))\n",
        "    plt.xlabel(\"EPOCHS\")\n",
        "    # naming the y axis\n",
        "    plt.ylabel(\"MSE\")\n",
        "    plt.legend()\n",
        "    # plt.show()\n",
        "  return w,J\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def SLP(df,W,alpha,rho,epochs,showPlot):\n",
        "\n",
        "  a = np.array(df.values)\n",
        "\n",
        "  X = a[:,0:len(a[0])-1]\n",
        "\n",
        "  Y = a[:,len(a[0])-1]\n",
        "\n",
        "  # X input Y output y for one hot encoding\n",
        "  # one hot encoding\n",
        "  y = []\n",
        "\n",
        "  for i in range(len(Y)):\n",
        "    yi = []\n",
        "    for j in range(len(classes)):\n",
        "      if(j==Y[i]):\n",
        "        yi.append(1)\n",
        "      else:\n",
        "        yi.append(0)\n",
        "\n",
        "    y.append(yi)\n",
        "\n",
        "  print(y)\n",
        "\n",
        "\n",
        "  mse_graph = []\n",
        "  epoch_graph = []\n",
        "\n",
        "  itr = 1\n",
        "\n",
        "  prevJ = 0\n",
        "  while itr <= epochs :\n",
        "\n",
        "    # df = df.sample(frac = 1)\n",
        "\n",
        "    a = np.array(df.values)\n",
        "\n",
        "    X = a[:,0:len(a[0])-1]\n",
        "\n",
        "    Y = a[:,len(a[0])-1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    currJ = 0\n",
        "    for k in range(len(X)) :\n",
        "\n",
        "      x = X[k]\n",
        "\n",
        "      d = []\n",
        "\n",
        "      for w in W:\n",
        "        d.append(logistic_predicted_sigmoid(x,w))\n",
        "\n",
        "      sum = 0\n",
        "\n",
        "      for j in range(len(d)):\n",
        "        sum += pow(d[j]-y[k][j],2)\n",
        "      \n",
        "      sum /= 2\n",
        "\n",
        "      currJ += sum\n",
        "\n",
        "\n",
        "      \n",
        "      for j in range(len(W)):\n",
        "        for i in range(len(W[j])):\n",
        "          W[j][i] = W[j][i] + alpha*(y[k][j] - d[j])*x[i]*d[j]*(1-d[j])\n",
        "\n",
        "    currJ = currJ/len(X)\n",
        "\n",
        "    epoch_graph.append(itr)\n",
        "    mse_graph.append(currJ)\n",
        "\n",
        "    # print('currJ : ',currJ)\n",
        "    # print('prevJ : ', prevJ)\n",
        "\n",
        "    # print(abs(currJ - prevJ))\n",
        "\n",
        "\n",
        "    if(abs(currJ - prevJ) < rho):\n",
        "      break\n",
        "\n",
        "    prevJ = currJ\n",
        "    \n",
        "    itr += 1\n",
        "\n",
        "  # print(epoch_graph)\n",
        "  if(showPlot != ''):\n",
        "\n",
        "    plt.plot(epoch_graph,mse_graph,label=str(showPlot))\n",
        "    plt.xlabel(\"EPOCHS\")\n",
        "    # naming the y axis\n",
        "    plt.ylabel(\"MSE\")\n",
        "    plt.legend()\n",
        "    # plt.show()\n",
        "  return W,currJ  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def findAccuracy(classes,df,everyClassW,datasetName):\n",
        "\n",
        "  # Confusion Matrix\n",
        "  confusionMatrix = np.zeros((len(classes),len(classes)),dtype='int')\n",
        "\n",
        "\n",
        "\n",
        "  a = np.array(df.values)\n",
        "\n",
        "  x = a[:,0:len(a[0])-1]\n",
        "\n",
        "  y = a[:,len(a[0])-1]\n",
        "\n",
        "  # for each pattern predict H(Xi) for all classes then take max probability\n",
        "\n",
        "  correctlyPredicted = 0\n",
        "\n",
        "  for i in range(len(a)):\n",
        "\n",
        "    predictedForIthPattern = []\n",
        "    for c in classes:\n",
        "      predictedForIthPattern.append(logistic_predicted_sigmoid(everyClassW[c],x[i]))\n",
        "    # print(predictedForIthPattern)\n",
        "\n",
        "    # predicting the class.\n",
        "    predictedClass = predictedForIthPattern.index(max(predictedForIthPattern))\n",
        "\n",
        "\n",
        "    confusionMatrix[predictedClass][int(y[i])] += 1\n",
        "\n",
        "    if(predictedClass == y[i]):\n",
        "      correctlyPredicted += 1\n",
        "\n",
        "  print('Correctly Predicted : ',correctlyPredicted)\n",
        "  print('Total '+ datasetName +  ' Samples : ', len(a))\n",
        "  print(datasetName + ' Accuracy : ',(correctlyPredicted*100)/len(a))\n",
        "\n",
        "  print()\n",
        "  print('confusionMatrix :' )\n",
        "  print(confusionMatrix)\n",
        "  print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(len(confusionMatrix)):\n",
        "    print('precision for ',i,' class : ',confusionMatrix[i][i]/(sum(confusionMatrix[i])))\n",
        "\n",
        "  print()\n",
        "\n",
        "  colWiseSum_ConfusionMatrix = np.sum(confusionMatrix,axis=0)\n",
        "  for i in range(len(colWiseSum_ConfusionMatrix)):\n",
        "    print('recall for ',i,' class : ',confusionMatrix[i][i]/(colWiseSum_ConfusionMatrix[i]))\n",
        "\n",
        "  \n",
        "  print('\\n\\n')\n",
        "\n",
        "  return (correctlyPredicted*100)/len(a)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "      \n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V9qlN5OHqkLe",
        "outputId": "46d82f7c-eff6-4664-8787-585c0043a18e"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "PRE-PROCCESSING STARTS\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "data = pd.DataFrame((datasets.load_iris()).data)\n",
        "target = pd.DataFrame((datasets.load_iris()).target)\n",
        "\n",
        "\n",
        "df = data;\n",
        "df = df/(df.max().max())\n",
        "df.insert(loc=len(df.columns),column = 'last',value=target)\n",
        "df.insert(loc=0,column='-1',value=[1 for i in range(len(df))])\n",
        "\n",
        "# shuffling DataFrame\n",
        "\n",
        "df = df.sample(frac = 1)\n",
        "\n",
        "'''\n",
        "\n",
        "for binary classification\n",
        "df = df[df['last']!=2]\n",
        "\n",
        "'''\n",
        "# df = df[df['last']!=2]\n",
        "\n",
        "# print(df)\n",
        "\n",
        "# print(df)\n",
        "\n",
        "# split the dataset in x:y:z\n",
        "\n",
        "x = 55\n",
        "y = 15\n",
        "z = 30\n",
        "train_set, reamining_set = train_test_split(df,train_size = x/100)\n",
        "validation_set, test_set = train_test_split(reamining_set,train_size = y/(y+z))\n",
        "print(len(train_set))\n",
        "print(len(validation_set))\n",
        "print(len(test_set))\n",
        "\n",
        "\n",
        "test_set = reamining_set\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "alphaarr = np.array([0.1,0.001,0.1,0.03,0.15])\n",
        "rhoarr = np.array([0.0001,0.011,0,0.001,0.0001])\n",
        "epocharr = np.array([100,200,1000,400,500])\n",
        "\n",
        "\n",
        "\n",
        "# df,w,alpha,rho,epochs\n",
        "\n",
        "\n",
        "'''\n",
        "PRE-PROCCESSING ENDS\n",
        "'''\n",
        "\n",
        "print()\n",
        "\n",
        "\n",
        "classes = np.unique(np.array(list(df['last'])))\n",
        "\n",
        "\n",
        "intialW = [random.uniform(-0.3,0.3) for i in range(len(df.columns)-1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "W = [intialW.copy() for i in range(len(classes)) ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "returnedW,J = SLP(train_set,W.copy(),0.1,0.00,1000,'ok')\n",
        "\n",
        "\n",
        "print('Train Set : ')\n",
        "findAccuracy(classes,train_set,returnedW.copy(),'TrainSet')\n",
        "findAccuracy(classes,test_set,returnedW.copy(),'TestSet')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(train_set)\n",
        "\n",
        "print(test_set)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "   \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82\n",
            "22\n",
            "46\n",
            "\n",
            "[[0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
            "Train Set : \n",
            "Correctly Predicted :  79\n",
            "Total TrainSet Samples :  82\n",
            "TrainSet Accuracy :  96.34146341463415\n",
            "\n",
            "confusionMatrix :\n",
            "[[34  0  0]\n",
            " [ 0 22  0]\n",
            " [ 0  3 23]]\n",
            "\n",
            "precision for  0  class :  1.0\n",
            "precision for  1  class :  1.0\n",
            "precision for  2  class :  0.8846153846153846\n",
            "\n",
            "recall for  0  class :  1.0\n",
            "recall for  1  class :  0.88\n",
            "recall for  2  class :  1.0\n",
            "\n",
            "\n",
            "\n",
            "Correctly Predicted :  60\n",
            "Total TestSet Samples :  68\n",
            "TestSet Accuracy :  88.23529411764706\n",
            "\n",
            "confusionMatrix :\n",
            "[[16  0  0]\n",
            " [ 0 19  2]\n",
            " [ 0  6 25]]\n",
            "\n",
            "precision for  0  class :  1.0\n",
            "precision for  1  class :  0.9047619047619048\n",
            "precision for  2  class :  0.8064516129032258\n",
            "\n",
            "recall for  0  class :  1.0\n",
            "recall for  1  class :  0.76\n",
            "recall for  2  class :  0.9259259259259259\n",
            "\n",
            "\n",
            "\n",
            "     -1         0         1         2         3  last\n",
            "89    1  0.696203  0.316456  0.506329  0.164557     1\n",
            "75    1  0.835443  0.379747  0.556962  0.177215     1\n",
            "17    1  0.645570  0.443038  0.177215  0.037975     0\n",
            "32    1  0.658228  0.518987  0.189873  0.012658     0\n",
            "35    1  0.632911  0.405063  0.151899  0.025316     0\n",
            "..   ..       ...       ...       ...       ...   ...\n",
            "60    1  0.632911  0.253165  0.443038  0.126582     1\n",
            "87    1  0.797468  0.291139  0.556962  0.164557     1\n",
            "132   1  0.810127  0.354430  0.708861  0.278481     2\n",
            "72    1  0.797468  0.316456  0.620253  0.189873     1\n",
            "14    1  0.734177  0.506329  0.151899  0.025316     0\n",
            "\n",
            "[82 rows x 6 columns]\n",
            "     -1         0         1         2         3  last\n",
            "137   1  0.810127  0.392405  0.696203  0.227848     2\n",
            "110   1  0.822785  0.405063  0.645570  0.253165     2\n",
            "74    1  0.810127  0.367089  0.544304  0.164557     1\n",
            "135   1  0.974684  0.379747  0.772152  0.291139     2\n",
            "109   1  0.911392  0.455696  0.772152  0.316456     2\n",
            "..   ..       ...       ...       ...       ...   ...\n",
            "122   1  0.974684  0.354430  0.848101  0.253165     2\n",
            "2     1  0.594937  0.405063  0.164557  0.025316     0\n",
            "112   1  0.860759  0.379747  0.696203  0.265823     2\n",
            "141   1  0.873418  0.392405  0.645570  0.291139     2\n",
            "142   1  0.734177  0.341772  0.645570  0.240506     2\n",
            "\n",
            "[68 rows x 6 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}